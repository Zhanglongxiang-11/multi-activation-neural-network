{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "moral-quest",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "selected-encyclopedia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    Y_1 = np.exp(X)\n",
    "    Y_sum = np.sum(Y_1, axis=1, keepdims=True)\n",
    "    Y = Y_1 / Y_sum\n",
    "    return Y, X\n",
    "\n",
    "def sigmoid(X):\n",
    "    Y = 1/(1 + np.exp(-X))\n",
    "    return Y,X\n",
    "\n",
    "def relu(X):\n",
    "    Y = np.maximum(0,X)\n",
    "    return Y,X\n",
    "\n",
    "def tanh(X):\n",
    "    Y = (np.exp(X) - np.exp(-X))/(np.exp(X) + np.exp(-X))\n",
    "    return Y,X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "opening-patch",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_sigmoid(dL, X):\n",
    "    Y, X = sigmoid(X)\n",
    "    dX = dL * Y * (1 - Y)\n",
    "    return dX\n",
    "\n",
    "def back_softmax(dL, X):\n",
    "    Y, X = softmax(X)\n",
    "    dX = dL * Y * (1 - Y)\n",
    "    return dX\n",
    "\n",
    "def back_relu(dL, X):\n",
    "    Y, X = relu(X)\n",
    "    p = (Y + abs(Y))\n",
    "    d = np.int64(p > 0)\n",
    "    dX = dL * d\n",
    "    return dX\n",
    "\n",
    "def back_tanh(dL, X):\n",
    "    Y, X = tanh(X)\n",
    "    dX = (1 - Y**2) * dL\n",
    "    return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "million-enzyme",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0]\n",
      " [2 0 2]\n",
      " [3 3 0]]\n"
     ]
    }
   ],
   "source": [
    "dL=np.array([[1,1,1],[2,2,2],[3,3,3]])\n",
    "x = np.array([[2,2,-3],[3,-3,3],[4,4,-4]])\n",
    "d = back_relu(dL, x)\n",
    "\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "based-synthesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims, activation_num=3):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L-1):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1])*0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n",
    "        parameters['mu' + str(l)] = np.zeros((layer_dims[l],activation_num))\n",
    "        parameters['phi' + str(l)] = softmax(parameters['mu' + str(l)])[0]\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "        assert (parameters['mu' + str(l)].shape == (layer_dims[l],activation_num))\n",
    "    \n",
    "    parameters['W' + str(L-1)] = np.random.randn(layer_dims[L-1],layer_dims[L-2])*0.01\n",
    "    parameters['b' + str(L-1)] = np.zeros((layer_dims[L-1],1))\n",
    "        \n",
    "                \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "descending-observer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': array([[ 0.01788628,  0.0043651 ,  0.00096497],\n",
      "       [-0.01863493, -0.00277388, -0.00354759],\n",
      "       [-0.00082741, -0.00627001, -0.00043818],\n",
      "       [-0.00477218, -0.01313865,  0.00884622],\n",
      "       [ 0.00881318,  0.01709573,  0.00050034]]), 'b1': array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), 'mu1': array([[0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.]]), 'phi1': array([[0.33333333, 0.33333333, 0.33333333],\n",
      "       [0.33333333, 0.33333333, 0.33333333],\n",
      "       [0.33333333, 0.33333333, 0.33333333],\n",
      "       [0.33333333, 0.33333333, 0.33333333],\n",
      "       [0.33333333, 0.33333333, 0.33333333]]), 'W2': array([[-0.00404677, -0.0054536 , -0.01546477,  0.00982367, -0.01101068]]), 'b2': array([[0.]])}\n",
      "[[0.24340108 0.22247347 0.25855134 0.28261095]\n",
      " [0.11362999 0.12817381 0.10775743 0.09225187]\n",
      " [0.16068755 0.15633608 0.15809612 0.15983603]\n",
      " [0.19861988 0.17944406 0.17192652 0.18125607]\n",
      " [0.21588179 0.22789945 0.2345089  0.23527911]]\n",
      "[[0.49887113 0.49880912 0.49875699 0.49876786]]\n",
      "()\n",
      "0.6931136219782339\n"
     ]
    }
   ],
   "source": [
    "layer_dims = [3,5,1]\n",
    "parameter = initialize_parameters_deep(layer_dims, activation_num=3)\n",
    "print(parameter)\n",
    "A1 = linear_activation_forward(X, parameter['W1'], parameter['b1'], parameter['mu1'])[0]\n",
    "print(A1)\n",
    "A2 = output_layer(A1, parameter['W2'], parameter['b2'])[0]\n",
    "print(A2)\n",
    "cost = compute_cost(A2,Y)\n",
    "print(cost)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "thrown-beijing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "   \n",
    "    ### START CODE HERE ### (â‰ˆ 1 line of code)\n",
    "    Z = np.dot(W, A) + b\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    \n",
    "\n",
    "    assert (Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "assisted-makeup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[65 62 62 69]\n",
      " [45 46 45 48]]\n",
      "(array([[5, 3, 6, 8],\n",
      "       [1, 3, 2, 1],\n",
      "       [9, 8, 7, 8]]), array([[3, 4, 5],\n",
      "       [2, 4, 3]]), array([[1],\n",
      "       [4]]))\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[5,3,6,8],[1,3,2,1],[9,8,7,8]])\n",
    "W1 = np.array([[3,4,5],[2,4,3]])\n",
    "b1 = np.array([[1],[4]])\n",
    "print(linear_forward(X,W1,b1)[0])\n",
    "print(linear_forward(X,W1,b1)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "independent-start",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, mu):\n",
    "    \n",
    "\n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "    A_1, activation_cache_1 = relu(Z)\n",
    "    A_2, activation_cache_2 = sigmoid(Z)\n",
    "    A_3, activation_cache_3 = tanh(Z)\n",
    "    phi, mu_cache = softmax(mu)\n",
    "    A = A_1 * phi[:,0].reshape((-1,1)) + A_2 * phi[:,1].reshape((-1,1)) + A_3 * phi[:,2].reshape((-1,1))\n",
    "\n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache_1, mu_cache, phi)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dedicated-hebrew",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16.66262215 15.92843673 15.92843673 17.64153603]\n",
      " [11.76805273 12.0127812  11.76805273 12.50223814]]\n",
      "((array([[5, 3, 6, 8],\n",
      "       [1, 3, 2, 1],\n",
      "       [9, 8, 7, 8]]), array([[3, 4, 5],\n",
      "       [2, 4, 3]]), array([[1],\n",
      "       [4]])), array([[65, 62, 62, 69],\n",
      "       [45, 46, 45, 48]]), array([[4, 3, 5],\n",
      "       [2, 3, 1]]), array([[0.24472847, 0.09003057, 0.66524096],\n",
      "       [0.24472847, 0.66524096, 0.09003057]]))\n",
      "(2, 4)\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[5,3,6,8],[1,3,2,1],[9,8,7,8]])\n",
    "W1 = np.array([[3,4,5],[2,4,3]])\n",
    "b1 = np.array([[1],[4]])\n",
    "mu1 = np.array([[4,3,5],[2,3,1]])\n",
    "\n",
    "A1, cache = linear_activation_forward(X,W1,b1,mu1)\n",
    "\n",
    "print(A1)\n",
    "print(cache)\n",
    "print(A1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "lesbian-nation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_layer(A_prev, W, b):\n",
    "   \n",
    "\n",
    "\n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "    A, activation_cache = sigmoid(Z)\n",
    "  \n",
    "\n",
    "\n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "oriented-empty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16.66262215 15.92843673 15.92843673 17.64153603]\n",
      " [11.76805273 12.0127812  11.76805273 12.50223814]]\n",
      "[[0.68652924 0.67105742 0.67051698 0.7087201 ]]\n"
     ]
    }
   ],
   "source": [
    "W2 = np.array([[0.1,0.01]])\n",
    "\n",
    "print(A1)\n",
    "\n",
    "b2 = -1\n",
    "print(output_layer(A1,W2,b2)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "limited-chester",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 4 + 1  # number of layers in the neural network\n",
    "\n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        ### START CODE HERE ### (â‰ˆ 2 lines of code)\n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)],\n",
    "                                             parameters['mu' + str(l)])\n",
    "        caches.append(cache)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    ### START CODE HERE ### (â‰ˆ 2 lines of code)\n",
    "    AL, cache = output_layer(A, parameters['W' + str(L)], parameters['b' + str(L)])\n",
    "    caches.append(cache)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    #assert (AL.shape == (1, X.shape[1]))\n",
    "\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "running-arcade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49887113 0.49880912 0.49875699 0.49876786]]\n"
     ]
    }
   ],
   "source": [
    "AL, caches = L_model_forward(X, parameter)\n",
    "print(AL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "married-rebecca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((array([[5, 3, 6, 8],\n",
      "       [1, 3, 2, 1],\n",
      "       [9, 8, 7, 8]]), array([[3, 4, 5],\n",
      "       [2, 4, 3]]), array([[1],\n",
      "       [4]])), array([[65, 62, 62, 69],\n",
      "       [45, 46, 45, 48]]), array([[4, 3, 5],\n",
      "       [2, 3, 1]]), array([[0.24472847, 0.09003057, 0.66524096],\n",
      "       [0.24472847, 0.66524096, 0.09003057]])), ((array([[16.66262215, 15.92843673, 15.92843673, 17.64153603],\n",
      "       [11.76805273, 12.0127812 , 11.76805273, 12.50223814]]), array([[0.1 , 0.01]]), array([[-1]])), array([[0.78394274, 0.71297149, 0.7105242 , 0.88917598]]))]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[5,3,6,8],[1,3,2,1],[9,8,7,8]])\n",
    "W1 = np.array([[3,4,5],[2,4,3]])\n",
    "b1 = np.array([[1],[4]])\n",
    "mu1 = np.array([[4,3,5],[2,3,1]])\n",
    "phi1 = softmax(mu1)[0]\n",
    "W2 = np.array([[0.1,0.01]])\n",
    "b2 = np.array([[-1]])\n",
    "mu2 = 0\n",
    "phi2 = 0\n",
    "parameters = {'W1':W1,'b1':b1,'mu1':mu1,'phi1':phi1,'W2':W2,'b2':b2,'mu2':mu2,'phi2':phi2}\n",
    "\n",
    "\n",
    "y_hat = L_model_forward(X, parameters)[0]\n",
    "total_cache = L_model_forward(X, parameters)[1]\n",
    "print(total_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "rural-trinity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "   \n",
    "\n",
    "    m = Y.shape[1]\n",
    "    # Compute loss from aL and y.\n",
    "    ### START CODE HERE ### (â‰ˆ 1 lines of code)\n",
    "    cost = -1 / m * np.sum(Y * np.log(AL) + (1 - Y) * np.log(1 - AL), axis=1, keepdims=True)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    cost = np.squeeze(cost)\n",
    "    # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert (cost.shape == ())\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dedicated-citizenship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "0.7356258984305145\n"
     ]
    }
   ],
   "source": [
    "Y = np.array([[1,0,0,1]])\n",
    "cost = compute_cost(y_hat,Y)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "detailed-emphasis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    " \n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    ### START CODE HERE ### (â‰ˆ 3 lines of code)\n",
    "    dW = 1 / m * np.dot(dZ, A_prev.T)\n",
    "    db = 1 / m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "undefined-wealth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_backward(dA, cache):\n",
    "    \n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    dZ = back_sigmoid(dA, activation_cache)\n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "magnetic-preview",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.45660219  3.04004422  3.03505776 -1.41099428]]\n"
     ]
    }
   ],
   "source": [
    "dA2 = - (np.divide(Y, y_hat) - np.divide(1 - Y, 1 - y_hat))\n",
    "print(dA2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "affected-radical",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((array([[16.66262215, 15.92843673, 15.92843673, 17.64153603],\n",
      "       [11.76805273, 12.0127812 , 11.76805273, 12.50223814]]), array([[0.1 , 0.01]]), array([[-1]])), array([[0.78394274, 0.71297149, 0.7105242 , 0.88917598]]))\n",
      "[[-0.03134708  0.06710574  0.0670517  -0.02912799]\n",
      " [-0.00313471  0.00671057  0.00670517 -0.0029128 ]]\n",
      "[[2.75182829 2.15533849]]\n",
      "[[0.18420593]]\n"
     ]
    }
   ],
   "source": [
    "cache = total_cache[1]\n",
    "print(cache)\n",
    "\n",
    "dA_1, dw, db = first_backward(dA2, cache)\n",
    "print(dA_1)\n",
    "print(dw)\n",
    "print(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "round-proof",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache, mu, phi = cache\n",
    "    m = dA.shape[1]\n",
    "    \n",
    "    A_1 = relu(activation_cache)[0]\n",
    "    A_2 = sigmoid(activation_cache)[0]\n",
    "    A_3 = tanh(activation_cache)[0]\n",
    "    \n",
    "    dphi = np.zeros(phi.shape)\n",
    "    dphi[:,0] = 1 / m * np.sum(A_1 * dA, axis=1, keepdims=True).reshape((1,-1))\n",
    "    dphi[:,1] = 1 / m * np.sum(A_2 * dA, axis=1, keepdims=True).reshape((1,-1))\n",
    "    dphi[:,2] = 1 / m * np.sum(A_3 * dA, axis=1, keepdims=True).reshape((1,-1))\n",
    "    \n",
    "    dmu = back_softmax(dphi, mu)\n",
    "    \n",
    "    dA_1 = dA * phi[:,0].reshape((-1,1))\n",
    "    dA_2 = dA * phi[:,1].reshape((-1,1))\n",
    "    dA_3 = dA * phi[:,2].reshape((-1,1))\n",
    "    \n",
    "    dZ = 1 / 3 * (back_relu(dA_1, activation_cache) + back_sigmoid(dA_2, activation_cache) + back_tanh(dA_3, activation_cache))\n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    \n",
    "    return dA_prev, dW, db, dmu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "written-surveillance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[64.5  1.   1. ]\n",
      " [46.   1.   1. ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"A_1:\",A_1)\\nprint(\\'A_2:\\',A_2)\\nprint(\\'A_3:\\',A_3)\\nprint(\\'mu:\\',mu)\\nprint(\\'phi:\\',phi)\\nprint(np.array([A_1, A_2, A_3]))\\nprint(phi[0])\\nprint(A_1 * phi[:,0].reshape((-1,1)))\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache = L_model_forward(X, parameters)[1][0]\n",
    "linear_cache, activation_cache, mu, phi = cache\n",
    "    \n",
    "A_1 = relu(activation_cache)[0]\n",
    "A_2 = sigmoid(activation_cache)[0]\n",
    "A_3 = tanh(activation_cache)[0]\n",
    "\n",
    "dphi = np.zeros(phi.shape)\n",
    "dphi[:,0] = 1 / 4 * np.sum(A_1, axis=1, keepdims=True).reshape((1,-1))\n",
    "dphi[:,1] = 1 / 4 * np.sum(A_2, axis=1, keepdims=True).reshape((1,-1))\n",
    "dphi[:,2] = 1 / 4 * np.sum(A_3, axis=1, keepdims=True).reshape((1,-1))\n",
    "\n",
    "print(dphi)\n",
    "dmu = back_softmax(dphi, mu)\n",
    "#dphi = dA * np.array([A_1, A_2, A_3])\n",
    "#dA_1 = dA * phi[0]\n",
    "#dA_2 = dA * phi[1]\n",
    "#dA_3 = dA * phi[2]\n",
    "'''\n",
    "print(\"A_1:\",A_1)\n",
    "print('A_2:',A_2)\n",
    "print('A_3:',A_3)\n",
    "print('mu:',mu)\n",
    "print('phi:',phi)\n",
    "print(np.array([A_1, A_2, A_3]))\n",
    "print(phi[0])\n",
    "print(A_1 * phi[:,0].reshape((-1,1)))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "nasty-flight",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "\n",
    "    # Initializing the backpropagation\n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    ### START CODE HERE ### (approx. 2 lines)\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = first_backward(dAL, current_cache)\n",
    "    ### END CODE HERE ###\n",
    "      \n",
    "    for l in reversed(range(L - 1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 2)], caches\". Outputs: \"grads[\"dA\" + str(l + 1)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        ### START CODE HERE ### (approx. 5 lines)\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp,dmu_temp = linear_activation_backward(grads[\"dA\" + str(l+2)], current_cache)\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "        grads['dmu' + str(l + 1)] = dmu_temp\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "confidential-grass",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \n",
    "    L = len(parameters) // 4 + 1 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    ### START CODE HERE ### (â‰ˆ 3 lines of code)\n",
    "    for l in range(L-1):\n",
    "        parameters[\"W\" + str(l+1)] =  parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
    "        parameters[\"mu\" + str(l+1)] = parameters[\"mu\" + str(l+1)] - learning_rate * grads[\"dmu\" + str(l + 1)]\n",
    "        parameters[\"phi\" + str(l+1)] = softmax(parameters[\"mu\" + str(l+1)])[0]\n",
    "        \n",
    "    parameters[\"W\" + str(L)] =  parameters[\"W\" + str(L)] - learning_rate * grads[\"dW\" + str(L)]\n",
    "    parameters[\"b\" + str(L)] = parameters[\"b\" + str(L)] - learning_rate * grads[\"db\" + str(L)]\n",
    "    ### END CODE HERE ###\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "indie-wrist",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization.\n",
    "    ### START CODE HERE ###\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        ### START CODE HERE ### (â‰ˆ 1 line of code)\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Compute cost.\n",
    "        ### START CODE HERE ### (â‰ˆ 1 line of code)\n",
    "        cost = compute_cost(AL, Y)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "        # Backward propagation.\n",
    "        ### START CODE HERE ### (â‰ˆ 1 line of code)\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        ### END CODE HERE ###\n",
    " \n",
    "        # Update parameters.\n",
    "        ### START CODE HERE ### (â‰ˆ 1 line of code)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        ### END CODE HERE ###\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "fabulous-sunday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693114\n",
      "Cost after iteration 100: 0.680856\n",
      "Cost after iteration 200: 0.497173\n",
      "Cost after iteration 300: 0.191859\n",
      "Cost after iteration 400: 0.074378\n",
      "Cost after iteration 500: 0.040018\n",
      "Cost after iteration 600: 0.025787\n",
      "Cost after iteration 700: 0.018259\n",
      "Cost after iteration 800: 0.013903\n",
      "Cost after iteration 900: 0.011037\n",
      "Cost after iteration 1000: 0.009049\n",
      "Cost after iteration 1100: 0.007623\n",
      "Cost after iteration 1200: 0.006527\n",
      "Cost after iteration 1300: 0.005691\n",
      "Cost after iteration 1400: 0.005027\n",
      "Cost after iteration 1500: 0.004480\n",
      "Cost after iteration 1600: 0.004034\n",
      "Cost after iteration 1700: 0.003660\n",
      "Cost after iteration 1800: 0.003342\n",
      "Cost after iteration 1900: 0.003071\n",
      "Cost after iteration 2000: 0.002836\n",
      "Cost after iteration 2100: 0.002632\n",
      "Cost after iteration 2200: 0.002454\n",
      "Cost after iteration 2300: 0.002293\n",
      "Cost after iteration 2400: 0.002151\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.array([[5,3,6,8],[1,3,2,1],[9,8,7,8]])\n",
    "Y = np.array([[1,0,0,1]])\n",
    "layer_dims = [3,5,1]\n",
    "\n",
    "parameters = L_layer_model(X, Y, layer_dims, learning_rate = 0.1, num_iterations = 2500, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "fewer-ecology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': array([[ 0.07015499, -1.10253819,  0.27013753],\n",
      "       [-0.01317266,  0.63310492, -0.0707708 ],\n",
      "       [ 0.01334742,  0.3025798 ,  0.00373733],\n",
      "       [ 0.05195934, -1.55313614,  0.42084965],\n",
      "       [-0.0512721 ,  1.48307555, -0.15414777]]), 'b1': array([[-0.1094455 ],\n",
      "       [ 0.0696564 ],\n",
      "       [ 0.03702329],\n",
      "       [-0.15182629],\n",
      "       [ 0.15889575]]), 'mu1': array([[0.82902401, 0.10854565, 0.60150856],\n",
      "       [0.20665696, 0.08464309, 0.19248039],\n",
      "       [0.04423145, 0.02859672, 0.04473525],\n",
      "       [1.68280727, 0.20856724, 1.06707254],\n",
      "       [1.43025977, 0.314197  , 0.90497369]]), 'phi1': array([[0.43801447, 0.21310257, 0.34888296],\n",
      "       [0.34830347, 0.30829597, 0.34340056],\n",
      "       [0.33500941, 0.32981236, 0.33517823],\n",
      "       [0.56522835, 0.12941053, 0.30536112],\n",
      "       [0.52111747, 0.17070087, 0.30818166]]), 'W2': array([[ 2.20025288, -1.20825876, -0.59448072,  3.14552401, -2.92435126]]), 'b2': array([[-0.6315523]])}\n"
     ]
    }
   ],
   "source": [
    "print(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "neural-payment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[0.43801447, 0.21310257, 0.34888296],\n",
      "       [0.34830347, 0.30829597, 0.34340056],\n",
      "       [0.33500941, 0.32981236, 0.33517823],\n",
      "       [0.56522835, 0.12941053, 0.30536112],\n",
      "       [0.52111747, 0.17070087, 0.30818166]]), array([[0.82902401, 0.10854565, 0.60150856],\n",
      "       [0.20665696, 0.08464309, 0.19248039],\n",
      "       [0.04423145, 0.02859672, 0.04473525],\n",
      "       [1.68280727, 0.20856724, 1.06707254],\n",
      "       [1.43025977, 0.314197  , 0.90497369]]))\n"
     ]
    }
   ],
   "source": [
    "print(softmax(parameters['mu1']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
